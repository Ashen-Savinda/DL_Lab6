{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwwq0nSdmsOL"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p2pHdwnsu7-",
        "outputId": "7658852c-d784-4dd1-9f60-89b1dfe44a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7gMjMCT677n"
      },
      "source": [
        "## Installing dependencies\n",
        "\n",
        "The installation of PyG on Colab can be a little bit tricky. Execute the cell below -- in case of issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html).\n",
        "\n",
        "_Note: This cell might take a while in some cases."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch-scatter torch-sparse torch-geometric  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nguIVY73fEg5",
        "outputId": "5f8f4df9-9e75-4264-95a2-4a2ae9e3c52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.4.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=537340 sha256=c8459ed7469d208085c56d0511d9f2873e8ce2186c18ba48b095c97613b7a1d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.4.1+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Building wheels for collected packages: torch-sparse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLxnaKsN8GVf"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qywlcjyr8USw"
      },
      "outputs": [],
      "source": [
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualization function for NX graph or PyTorch tensor\n",
        "def visualize(h, color, epoch=None, loss=None, accuracy=None):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    if torch.is_tensor(h):\n",
        "        h = h.detach().cpu().numpy()\n",
        "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "        if epoch is not None and loss is not None and accuracy['train'] is not None and accuracy['val'] is not None:\n",
        "            plt.xlabel((f'Epoch: {epoch}, Loss: {loss.item():.4f} \\n'\n",
        "                       f'Training Accuracy: {accuracy[\"train\"]*100:.2f}% \\n'\n",
        "                       f' Validation Accuracy: {accuracy[\"val\"]*100:.2f}%'),\n",
        "                       fontsize=16)\n",
        "    else:\n",
        "        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=False,\n",
        "                         node_color=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbny-iTO7NQN"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Recently, deep learning on graphs has emerged to one of the hottest research fields in the deep learning community.\n",
        "Here, **Graph Neural Networks (GNNs)** aim to generalize classical deep learning concepts to irregular structured data (in contrast to images or texts) and to enable neural networks to reason about objects and their relations.\n",
        "\n",
        "This tutorial will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the **[PyTorch Geometric (PyG) library](https://github.com/rusty1s/pytorch_geometric)**.\n",
        "PyTorch Geometric is an extension library to the popular deep learning framework [PyTorch](https://pytorch.org/), and consists of various methods and utilities to ease the implementation of Graph Neural Networks.\n",
        "\n",
        "Following [Kipf et al. (2017)](https://arxiv.org/abs/1609.02907), let's dive into the world of GNNs by looking at a simple graph-structured example, the well-known [**Zachary's karate club network**](https://en.wikipedia.org/wiki/Zachary%27s_karate_club). This graph describes a social network of 34 members of a karate club and documents links between members who interacted outside the club. Here, we are interested in detecting communities that arise from the member's interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3uPffzbyqn9"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "PyTorch Geometric provides an easy access to the dataset via the [`torch_geometric.datasets`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets) subpackage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrpL9CtS7nx2"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import KarateClub\n",
        "\n",
        "dataset = KarateClub()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCeRGa2q7sdl"
      },
      "source": [
        "After initializing the [`KarateClub`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.KarateClub) dataset, we first can inspect some of its properties.\n",
        "For example, we can see that this dataset holds exactly **one graph**, and that each node in this dataset is assigned a **34-dimensional feature vector** (which uniquely describes the members of the karate club).\n",
        "Furthermore, the graph holds exactly **4 classes**, which represent the community each node belongs to.\n",
        "\n",
        "Let's now look at the underlying graph in more detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTLapYhP7uCn"
      },
      "outputs": [],
      "source": [
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print(data)\n",
        "print('==============================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "#diameter of this dataset is 5.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {(2*data.num_edges) / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "important to note that **we use only 4 nodes during the training** in semi-suervised approach. But features/information from all nodes contribute but the known labels are only from these 4 nodes."
      ],
      "metadata": {
        "id": "2iXGaHlQvqMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.x # features are the one-hot-encoding of each node"
      ],
      "metadata": {
        "id": "6Ex1BlHmhSZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.y"
      ],
      "metadata": {
        "id": "TbEYZ7hhlrZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu7b7G6Ai3D1"
      },
      "outputs": [],
      "source": [
        "data.edge_index.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIzbIoc-y8J4"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5zhmKIH72Rf"
      },
      "source": [
        "Each graph in PyTorch Geometric is represented by a single [`Data`](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data) object, which holds all the information to describe its graph representation.\n",
        "We can print the data object anytime via `print(data)` to receive a short summary about its attributes and their shapes:\n",
        "```\n",
        "Data(edge_index=[2, 156], x=[34, 34], y=[34], train_mask=[34])\n",
        "```\n",
        "We can see that this `data` object holds 4 attributes:\n",
        "(1) The `edge_index` property holds the information about the **graph connectivity**, *i.e.*, a tuple of source and destination node indices for each edge.\n",
        "PyG further refers to (2) **node features** as `x` (each of the 34 nodes is assigned a 34-dim feature vector), and to (3) **node labels** as `y` (each node is assigned to exactly one class).\n",
        "(4) There also exists an additional attribute called `train_mask`, which describes for which nodes we already know their community assigments.\n",
        "In total, we are only aware of the ground-truth labels of 4 nodes (one for each community), and the task is to infer the community assignment for the remaining nodes.\n",
        "\n",
        "The `data` object also provides some **utility functions** to infer some basic properties of the underlying graph.\n",
        "For example, we can easily infer whether there exists isolated nodes in the graph (*i.e.* there exists no edge to any node), whether the graph contains self-loops (*i.e.*, $(v, v) \\in \\mathcal{E}$), or whether the graph is undirected (*i.e.*, for each edge $(v, w) \\in \\mathcal{E}$ there also exists the edge $(w, v) \\in \\mathcal{E}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WBDduWreQNQ"
      },
      "outputs": [],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLLsT0ROzffp"
      },
      "source": [
        "## Edge Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svWvpFyZeXfm"
      },
      "source": [
        "Next we'll print the `edge_index` of our graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFA6Xi4O79r0"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "edge_index = data.edge_index\n",
        "print(edge_index.t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQJyi9OB8dh_"
      },
      "source": [
        "By printing `edge_index`, we can further understand how PyG represents graph connectivity internally.\n",
        "We can see that for each edge, `edge_index` holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node of an edge.\n",
        "\n",
        "This representation is known as the **COO format (coordinate format)** commonly used for representing sparse matrices.\n",
        "Instead of holding the adjacency information in a dense representation $\\mathbf{A} \\in \\{ 0, 1 \\}^{|\\mathcal{V}| \\times |\\mathcal{V}|}$, PyG represents graphs sparsely, which refers to only holding the coordinates/values for which entries in $\\mathbf{A}$ are non-zero.\n",
        "\n",
        "We can further visualize the graph by converting it to the `networkx` library format, which implements, in addition to graph manipulation functionalities, powerful tools for visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KfJHtlV8h3W"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "visualize(G, color=data.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUdHZY2u8vn3"
      },
      "source": [
        "## Implementing Graph Neural Networks (GNNs)\n",
        "\n",
        "After learning about PyG's data handling, it's time to implement our first Graph Neural Network!\n",
        "\n",
        "For this, we will use one of the most simple GNN operators, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)).\n",
        "\n",
        "PyG implements this layer via [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv), which can be executed by passing in the node feature representation `x` and the COO graph connectivity representation `edge_index`.\n",
        "\n",
        "### What is the output of a GNN?\n",
        "\n",
        "The goal of a GNN is to take an input graph $G = (\\mathcal{V}, \\mathcal{E})$ where each node $v_i \\in \\mathcal{V}$ has an input feature vector $X_i^{(0)}$. What we want to learn is a function $f \\to \\mathcal{V} \\times \\mathcal{R}^d$, a function that takes in a node and its feature vector, as well as the graph structure, and outputs an _embedding_, a vector that represents that node in a way that's useful to our downstream task. Once we've mapped nodes and their initial features to their learned embeddings, we can use those embeddings to do a variety of different tasks including node-level, edge-level, or graph-level regression/classification.\n",
        "\n",
        "In this colab, we want to learn embeddings that will be useful to classify each node into its community.\n",
        "\n",
        "With this, we are ready to create our first Graph Neural Network by defining our network architecture in a `torch.nn.Module` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tQGQF8r8zIr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234)\n",
        "        self.tanh = nn.Tanh()\n",
        "        # Define 8 GCNConv layers\n",
        "        self.conv1 = GCNConv(dataset.num_features, 4)\n",
        "        self.conv2 = GCNConv(4, 4)\n",
        "        self.conv3 = GCNConv(4, 4)\n",
        "        self.conv4 = GCNConv(4, 4)\n",
        "        self.conv5 = GCNConv(4, 4)\n",
        "        self.conv6 = GCNConv(4, 4)\n",
        "        self.conv7 = GCNConv(4, 4)\n",
        "        self.conv8 = GCNConv(4, 2)  # Last layer has output features for classification\n",
        "\n",
        "        self.classifier = Linear(2, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply the 8 layers\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = self.tanh(h)\n",
        "\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = self.tanh(h)\n",
        "\n",
        "        h = self.conv3(h, edge_index)\n",
        "        h = self.tanh(h)\n",
        "\n",
        "        h = self.conv4(h, edge_index)\n",
        "        h = self.tanh(h)\n",
        "\n",
        "        h = self.conv5(h, edge_index)\n",
        "        h = self.tanh(h)\n",
        "\n",
        "        h = self.conv6(h, edge_index)\n",
        "        h = self.tanh(h)\n",
        "\n",
        "        h = self.conv7(h, edge_index)\n",
        "        h = self.tanh(h)\n",
        "\n",
        "        h = self.conv8(h, edge_index)\n",
        "        h = self.tanh(h)  # Final GNN embedding space.\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "        out = self.classifier(h)\n",
        "\n",
        "        return out, h\n",
        "\n",
        "model = GCN()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zgbaD5P8_M_"
      },
      "source": [
        "Here, we first initialize all of our building blocks in `__init__` and define the computation flow of our network in `forward`.\n",
        "We first define and stack **three graph convolution layers**. Each layer corresponds to aggregating information from each node's 1-hop neighborhood (its direct neighbors), but when we compose the layers together, we are able to aggregate information from each node's 3-hop neighborhood (all nodes up to 3 \"hops\" away).\n",
        "\n",
        "In addition, the `GCNConv` layers reduce the node feature dimensionality to $2$, *i.e.*, $34 \\rightarrow 4 \\rightarrow 4 \\rightarrow 2$. Each `GCNConv` layer is enhanced by a [tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html?highlight=tanh#torch.nn.Tanh) non-linearity.\n",
        "\n",
        "After that, we apply a single linear transformation ([`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear)) that acts as a classifier to map our nodes to 1 out of the 4 classes/communities.\n",
        "\n",
        "We return both the output of the final classifier as well as the final node embeddings produced by our GNN.\n",
        "We proceed to initialize our final model via `GCN()`, and printing our model produces a summary of all its used sub-modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48uhs_0j9AMX"
      },
      "outputs": [],
      "source": [
        "model = GCN()\n",
        "\n",
        "_, h = model(data.x, data.edge_index)\n",
        "print(f'Embedding shape: {list(h.shape)}')\n",
        "\n",
        "visualize(h, color=data.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDtJ9Zjw9I_Y"
      },
      "source": [
        "Remarkably, even before training the weights of our model, the model produces an embedding of nodes that closely resembles the community-structure of the graph.\n",
        "Nodes of the same color (community) are already closely clustered together in the embedding space, although the weights of our model are initialized **completely at random** and we have not yet performed any training so far!\n",
        "This leads to the conclusion that GNNs introduce a strong inductive bias, leading to similar embeddings for nodes that are close to each other in the input graph.\n",
        "\n",
        "### Training on the Karate Club Network\n",
        "\n",
        "But can we do better? Let's look at an example on how to train our network parameters based on the knowledge of the community assignments of 4 nodes in the graph (one for each community):\n",
        "\n",
        "Since everything in our model is differentiable and parameterized, we can add some labels, train the model and observe how the embeddings react.\n",
        "Here, we make use of a **semi-supervised or transductive learning procedure**: We simply train against one node per class, but are allowed to make use of the complete input graph data.\n",
        "\n",
        "Training our model is very similar to any other PyTorch model.\n",
        "In addition to defining our network architecture, we define a loss critertion (here, [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) and initialize a stochastic gradient optimizer (here, [`Adam`](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam)).\n",
        "After that, we perform multiple rounds of optimization, where each round consists of a forward and backward pass to compute the gradients of our model parameters w.r.t. to the loss derived from the forward pass.\n",
        "If you are not new to PyTorch, this scheme should appear familar to you.\n",
        "Otherwise, the PyTorch docs provide [a good introduction on how to train a neural network in PyTorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer).\n",
        "\n",
        "Note that our semi-supervised learning scenario is achieved by the following line:\n",
        "```\n",
        "loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "```\n",
        "While we compute node embeddings for all of our nodes, we **only make use of the training nodes for computing the loss**.\n",
        "Here, this is implemented by filtering the output of the classifier `out` and ground-truth labels `data.y` to only contain the nodes in the `train_mask`.\n",
        "\n",
        "Let us now start training and see how our node embeddings evolve over time (best experienced by explicitely running the code):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.train_mask # training nodes are the ones with 'True'"
      ],
      "metadata": {
        "id": "_5uM30_0vNWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI3DETGi9ND6"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 430})'''))\n",
        "\n",
        "model = GCN()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
        "\n",
        "def train(data):\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "    accuracy = {}\n",
        "    # Calculate training accuracy on our four examples\n",
        "    predicted_classes = torch.argmax(out[data.train_mask], axis=1) # [0.6, 0.2, 0.7, 0.1] -> 2\n",
        "    target_classes = data.y[data.train_mask]\n",
        "    accuracy['train'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "\n",
        "    # Calculate validation accuracy on the whole graph\n",
        "    predicted_classes = torch.argmax(out, axis=1)\n",
        "    target_classes = data.y\n",
        "    accuracy['val'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "\n",
        "    return loss, h, accuracy\n",
        "\n",
        "for epoch in range(500):\n",
        "    loss, h, accuracy = train(data)\n",
        "    # Visualize the node embeddings every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        visualize(h, color=data.y, epoch=epoch, loss=loss, accuracy=accuracy)\n",
        "        time.sleep(0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9bELRjibIRO"
      },
      "source": [
        "## Documentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmqyWVNObNcK"
      },
      "source": [
        "You can explore more PyG functions through its [documentation](https://pytorch-geometric.readthedocs.io/en/latest/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}